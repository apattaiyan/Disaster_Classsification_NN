{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a4079f",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83584c04",
   "metadata": {},
   "source": [
    "### Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5943464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist, bigrams\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcdbba5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Communal violence in Bhainsa, Telangana. \"Ston...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Telangana: Section 144 has been imposed in Bha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Morgantown, WV</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Lord Jesus, your love brings freedom and pard...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword        location  \\\n",
       "0   0  ablaze             NaN   \n",
       "1   1  ablaze             NaN   \n",
       "2   2  ablaze   New York City   \n",
       "3   3  ablaze  Morgantown, WV   \n",
       "4   4  ablaze             NaN   \n",
       "\n",
       "                                                text  target  \n",
       "0  Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
       "1  Telangana: Section 144 has been imposed in Bha...       1  \n",
       "2  Arsonist sets cars ablaze at dealership https:...       1  \n",
       "3  Arsonist sets cars ablaze at dealership https:...       1  \n",
       "4  \"Lord Jesus, your love brings freedom and pard...       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f102e995",
   "metadata": {},
   "source": [
    "### Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3feac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Communal violence in Bhainsa, Telangana. \"Ston...</td>\n",
       "      <td>1</td>\n",
       "      <td>Communal violence in Bhainsa Telangana Stones ...</td>\n",
       "      <td>[Communal, violence, in, Bhainsa, Telangana, S...</td>\n",
       "      <td>[communal, violence, in, bhainsa, telangana, s...</td>\n",
       "      <td>[communal, violence, bhainsa, telangana, stone...</td>\n",
       "      <td>[(communal, JJ), (violence, NN), (bhainsa, NN)...</td>\n",
       "      <td>[(communal, a), (violence, n), (bhainsa, n), (...</td>\n",
       "      <td>[communal, violence, bhainsa, telangana, stone...</td>\n",
       "      <td>communal violence bhainsa telangana stone pelt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Telangana: Section 144 has been imposed in Bha...</td>\n",
       "      <td>1</td>\n",
       "      <td>Telangana Section 144 has been imposed in Bhai...</td>\n",
       "      <td>[Telangana, Section, 144, has, been, imposed, ...</td>\n",
       "      <td>[telangana, section, 144, has, been, imposed, ...</td>\n",
       "      <td>[telangana, section, 144, imposed, bhainsa, ja...</td>\n",
       "      <td>[(telangana, JJ), (section, NN), (144, CD), (i...</td>\n",
       "      <td>[(telangana, a), (section, n), (144, None), (i...</td>\n",
       "      <td>[telangana, section, impose, bhainsa, january,...</td>\n",
       "      <td>telangana section impose bhainsa january clash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership</td>\n",
       "      <td>[Arsonist, sets, cars, ablaze, at, dealership]</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, at, dealership]</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, dealership]</td>\n",
       "      <td>[(arsonist, JJ), (sets, NNS), (cars, NNS), (ab...</td>\n",
       "      <td>[(arsonist, a), (sets, n), (cars, n), (ablaze,...</td>\n",
       "      <td>[arsonist, set, car, ablaze, dealership]</td>\n",
       "      <td>arsonist set car ablaze dealership</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Morgantown, WV</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership</td>\n",
       "      <td>[Arsonist, sets, cars, ablaze, at, dealership]</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, at, dealership]</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, dealership]</td>\n",
       "      <td>[(arsonist, JJ), (sets, NNS), (cars, NNS), (ab...</td>\n",
       "      <td>[(arsonist, a), (sets, n), (cars, n), (ablaze,...</td>\n",
       "      <td>[arsonist, set, car, ablaze, dealership]</td>\n",
       "      <td>arsonist set car ablaze dealership</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Lord Jesus, your love brings freedom and pard...</td>\n",
       "      <td>0</td>\n",
       "      <td>Lord Jesus your love brings freedom and pardon...</td>\n",
       "      <td>[Lord, Jesus, your, love, brings, freedom, and...</td>\n",
       "      <td>[lord, jesus, your, love, brings, freedom, and...</td>\n",
       "      <td>[lord, jesus, love, brings, freedom, pardon, f...</td>\n",
       "      <td>[(lord, NN), (jesus, NN), (love, VBP), (brings...</td>\n",
       "      <td>[(lord, n), (jesus, n), (love, v), (brings, n)...</td>\n",
       "      <td>[lord, jesus, love, brings, freedom, pardon, f...</td>\n",
       "      <td>lord jesus love brings freedom pardon fill hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>OC</td>\n",
       "      <td>If this child was Chinese, this tweet would ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>If this child was Chinese this tweet would hav...</td>\n",
       "      <td>[If, this, child, was, Chinese, this, tweet, w...</td>\n",
       "      <td>[if, this, child, was, chinese, this, tweet, w...</td>\n",
       "      <td>[child, chinese, tweet, would, gone, viral, so...</td>\n",
       "      <td>[(child, NN), (chinese, JJ), (tweet, NN), (wou...</td>\n",
       "      <td>[(child, n), (chinese, a), (tweet, n), (would,...</td>\n",
       "      <td>[child, chinese, tweet, go, viral, social, med...</td>\n",
       "      <td>child chinese tweet go viral social medium abl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, England</td>\n",
       "      <td>Several houses have been set ablaze in Ngemsib...</td>\n",
       "      <td>1</td>\n",
       "      <td>Several houses have been set ablaze in Ngemsib...</td>\n",
       "      <td>[Several, houses, have, been, set, ablaze, in,...</td>\n",
       "      <td>[several, houses, have, been, set, ablaze, in,...</td>\n",
       "      <td>[several, houses, set, ablaze, ngemsibaa, vill...</td>\n",
       "      <td>[(several, JJ), (houses, NNS), (set, VBD), (ab...</td>\n",
       "      <td>[(several, a), (houses, n), (set, v), (ablaze,...</td>\n",
       "      <td>[several, house, set, ablaze, ngemsibaa, villa...</td>\n",
       "      <td>several house set ablaze ngemsibaa village oku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Bharat</td>\n",
       "      <td>Asansol: A BJP office in Salanpur village was ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Asansol A BJP office in Salanpur village was s...</td>\n",
       "      <td>[Asansol, A, BJP, office, in, Salanpur, villag...</td>\n",
       "      <td>[asansol, a, bjp, office, in, salanpur, villag...</td>\n",
       "      <td>[asansol, bjp, office, salanpur, village, set,...</td>\n",
       "      <td>[(asansol, NNS), (bjp, JJ), (office, NN), (sal...</td>\n",
       "      <td>[(asansol, n), (bjp, a), (office, n), (salanpu...</td>\n",
       "      <td>[asansol, bjp, office, salanpur, village, set,...</td>\n",
       "      <td>asansol bjp office salanpur village set ablaze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Accra, Ghana</td>\n",
       "      <td>National Security Minister, Kan Dapaah's side ...</td>\n",
       "      <td>0</td>\n",
       "      <td>National Security Minister Kan Dapaahs side ch...</td>\n",
       "      <td>[National, Security, Minister, Kan, Dapaahs, s...</td>\n",
       "      <td>[national, security, minister, kan, dapaahs, s...</td>\n",
       "      <td>[national, security, minister, kan, dapaahs, s...</td>\n",
       "      <td>[(national, JJ), (security, NN), (minister, NN...</td>\n",
       "      <td>[(national, a), (security, n), (minister, n), ...</td>\n",
       "      <td>[national, security, minister, kan, dapaahs, s...</td>\n",
       "      <td>national security minister kan dapaahs side ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Searching</td>\n",
       "      <td>This creature who’s soul is no longer clarent ...</td>\n",
       "      <td>0</td>\n",
       "      <td>This creature whos soul is no longer clarent b...</td>\n",
       "      <td>[This, creature, whos, soul, is, no, longer, c...</td>\n",
       "      <td>[this, creature, whos, soul, is, no, longer, c...</td>\n",
       "      <td>[creature, whos, soul, longer, clarent, blue, ...</td>\n",
       "      <td>[(creature, NN), (whos, NN), (soul, NN), (long...</td>\n",
       "      <td>[(creature, n), (whos, n), (soul, n), (longer,...</td>\n",
       "      <td>[creature, soul, longer, clarent, blue, ablaze...</td>\n",
       "      <td>creature soul longer clarent blue ablaze thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword         location  \\\n",
       "0   0  ablaze              NaN   \n",
       "1   1  ablaze              NaN   \n",
       "2   2  ablaze    New York City   \n",
       "3   3  ablaze   Morgantown, WV   \n",
       "4   4  ablaze              NaN   \n",
       "5   5  ablaze               OC   \n",
       "6   6  ablaze  London, England   \n",
       "7   7  ablaze           Bharat   \n",
       "8   8  ablaze     Accra, Ghana   \n",
       "9   9  ablaze        Searching   \n",
       "\n",
       "                                                text  target  \\\n",
       "0  Communal violence in Bhainsa, Telangana. \"Ston...       1   \n",
       "1  Telangana: Section 144 has been imposed in Bha...       1   \n",
       "2  Arsonist sets cars ablaze at dealership https:...       1   \n",
       "3  Arsonist sets cars ablaze at dealership https:...       1   \n",
       "4  \"Lord Jesus, your love brings freedom and pard...       0   \n",
       "5  If this child was Chinese, this tweet would ha...       0   \n",
       "6  Several houses have been set ablaze in Ngemsib...       1   \n",
       "7  Asansol: A BJP office in Salanpur village was ...       1   \n",
       "8  National Security Minister, Kan Dapaah's side ...       0   \n",
       "9  This creature who’s soul is no longer clarent ...       0   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  Communal violence in Bhainsa Telangana Stones ...   \n",
       "1  Telangana Section 144 has been imposed in Bhai...   \n",
       "2           Arsonist sets cars ablaze at dealership    \n",
       "3          Arsonist sets cars ablaze at dealership     \n",
       "4  Lord Jesus your love brings freedom and pardon...   \n",
       "5  If this child was Chinese this tweet would hav...   \n",
       "6  Several houses have been set ablaze in Ngemsib...   \n",
       "7  Asansol A BJP office in Salanpur village was s...   \n",
       "8  National Security Minister Kan Dapaahs side ch...   \n",
       "9  This creature whos soul is no longer clarent b...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Communal, violence, in, Bhainsa, Telangana, S...   \n",
       "1  [Telangana, Section, 144, has, been, imposed, ...   \n",
       "2     [Arsonist, sets, cars, ablaze, at, dealership]   \n",
       "3     [Arsonist, sets, cars, ablaze, at, dealership]   \n",
       "4  [Lord, Jesus, your, love, brings, freedom, and...   \n",
       "5  [If, this, child, was, Chinese, this, tweet, w...   \n",
       "6  [Several, houses, have, been, set, ablaze, in,...   \n",
       "7  [Asansol, A, BJP, office, in, Salanpur, villag...   \n",
       "8  [National, Security, Minister, Kan, Dapaahs, s...   \n",
       "9  [This, creature, whos, soul, is, no, longer, c...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [communal, violence, in, bhainsa, telangana, s...   \n",
       "1  [telangana, section, 144, has, been, imposed, ...   \n",
       "2     [arsonist, sets, cars, ablaze, at, dealership]   \n",
       "3     [arsonist, sets, cars, ablaze, at, dealership]   \n",
       "4  [lord, jesus, your, love, brings, freedom, and...   \n",
       "5  [if, this, child, was, chinese, this, tweet, w...   \n",
       "6  [several, houses, have, been, set, ablaze, in,...   \n",
       "7  [asansol, a, bjp, office, in, salanpur, villag...   \n",
       "8  [national, security, minister, kan, dapaahs, s...   \n",
       "9  [this, creature, whos, soul, is, no, longer, c...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [communal, violence, bhainsa, telangana, stone...   \n",
       "1  [telangana, section, 144, imposed, bhainsa, ja...   \n",
       "2         [arsonist, sets, cars, ablaze, dealership]   \n",
       "3         [arsonist, sets, cars, ablaze, dealership]   \n",
       "4  [lord, jesus, love, brings, freedom, pardon, f...   \n",
       "5  [child, chinese, tweet, would, gone, viral, so...   \n",
       "6  [several, houses, set, ablaze, ngemsibaa, vill...   \n",
       "7  [asansol, bjp, office, salanpur, village, set,...   \n",
       "8  [national, security, minister, kan, dapaahs, s...   \n",
       "9  [creature, whos, soul, longer, clarent, blue, ...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(communal, JJ), (violence, NN), (bhainsa, NN)...   \n",
       "1  [(telangana, JJ), (section, NN), (144, CD), (i...   \n",
       "2  [(arsonist, JJ), (sets, NNS), (cars, NNS), (ab...   \n",
       "3  [(arsonist, JJ), (sets, NNS), (cars, NNS), (ab...   \n",
       "4  [(lord, NN), (jesus, NN), (love, VBP), (brings...   \n",
       "5  [(child, NN), (chinese, JJ), (tweet, NN), (wou...   \n",
       "6  [(several, JJ), (houses, NNS), (set, VBD), (ab...   \n",
       "7  [(asansol, NNS), (bjp, JJ), (office, NN), (sal...   \n",
       "8  [(national, JJ), (security, NN), (minister, NN...   \n",
       "9  [(creature, NN), (whos, NN), (soul, NN), (long...   \n",
       "\n",
       "                                         wordnet_pos  \\\n",
       "0  [(communal, a), (violence, n), (bhainsa, n), (...   \n",
       "1  [(telangana, a), (section, n), (144, None), (i...   \n",
       "2  [(arsonist, a), (sets, n), (cars, n), (ablaze,...   \n",
       "3  [(arsonist, a), (sets, n), (cars, n), (ablaze,...   \n",
       "4  [(lord, n), (jesus, n), (love, v), (brings, n)...   \n",
       "5  [(child, n), (chinese, a), (tweet, n), (would,...   \n",
       "6  [(several, a), (houses, n), (set, v), (ablaze,...   \n",
       "7  [(asansol, n), (bjp, a), (office, n), (salanpu...   \n",
       "8  [(national, a), (security, n), (minister, n), ...   \n",
       "9  [(creature, n), (whos, n), (soul, n), (longer,...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [communal, violence, bhainsa, telangana, stone...   \n",
       "1  [telangana, section, impose, bhainsa, january,...   \n",
       "2           [arsonist, set, car, ablaze, dealership]   \n",
       "3           [arsonist, set, car, ablaze, dealership]   \n",
       "4  [lord, jesus, love, brings, freedom, pardon, f...   \n",
       "5  [child, chinese, tweet, go, viral, social, med...   \n",
       "6  [several, house, set, ablaze, ngemsibaa, villa...   \n",
       "7  [asansol, bjp, office, salanpur, village, set,...   \n",
       "8  [national, security, minister, kan, dapaahs, s...   \n",
       "9  [creature, soul, longer, clarent, blue, ablaze...   \n",
       "\n",
       "                                          final_text  \n",
       "0  communal violence bhainsa telangana stone pelt...  \n",
       "1  telangana section impose bhainsa january clash...  \n",
       "2                 arsonist set car ablaze dealership  \n",
       "3                 arsonist set car ablaze dealership  \n",
       "4  lord jesus love brings freedom pardon fill hol...  \n",
       "5  child chinese tweet go viral social medium abl...  \n",
       "6  several house set ablaze ngemsibaa village oku...  \n",
       "7  asansol bjp office salanpur village set ablaze...  \n",
       "8  national security minister kan dapaahs side ch...  \n",
       "9  creature soul longer clarent blue ablaze thing...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import html\n",
    "\n",
    "# Function to remove URLs\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "# Function to remove emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Function to remove HTML tags\n",
    "def remove_html(text):\n",
    "    html_tag = re.compile(r'<.*?>')\n",
    "    return html_tag.sub(r'', text)\n",
    "\n",
    "# Function to remove punctuation (including single and double quotes)\n",
    "def remove_punct(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation + \"“”’‘\"))\n",
    "\n",
    "# Function to remove HTML entities\n",
    "def remove_html_entities(text):\n",
    "    return html.unescape(text)\n",
    "\n",
    "# Function to get POS tags in WordNet format\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Applying preprocessing functions\n",
    "df['text_clean'] = df['text'].apply(remove_URL)\n",
    "df['text_clean'] = df['text_clean'].apply(remove_emoji)\n",
    "df['text_clean'] = df['text_clean'].apply(remove_html)\n",
    "df['text_clean'] = df['text_clean'].apply(remove_html_entities)\n",
    "df['text_clean'] = df['text_clean'].apply(remove_punct)\n",
    "\n",
    "# Tokenizing the clean text\n",
    "df['tokenized'] = df['text_clean'].apply(word_tokenize)\n",
    "\n",
    "# Lowercasing the tokenized text\n",
    "df['lower'] = df['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "# Removing stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "df['stopwords_removed'] = df['lower'].apply(lambda x: [word for word in x if word not in stop])\n",
    "\n",
    "# Applying part of speech tags\n",
    "df['pos_tags'] = df['stopwords_removed'].apply(nltk.tag.pos_tag)\n",
    "\n",
    "# Converting parts of speech to WordNet format\n",
    "df['wordnet_pos'] = df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Applying lemmatization using the WordNet POS tags\n",
    "df['lemmatized'] = df['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x if tag is not None])\n",
    "\n",
    "# Removing stopwords again from the lemmatized output\n",
    "df['lemmatized'] = df['lemmatized'].apply(lambda x: [word for word in x if word not in stop])\n",
    "\n",
    "# Concatenating the lemmatized words into a single string\n",
    "df['final_text'] = df['lemmatized'].apply(' '.join)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac59f18",
   "metadata": {},
   "source": [
    "### Vectorisation (TF-IDF, CountVectorizer, Word2Vec, BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99fe050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1dcae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['final_text'], df['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c98a18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf5d542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb01a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Vectorization\n",
    "sentences = [text.split() for text in df['final_text']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "def get_word2vec_embeddings(text, model, vector_size):\n",
    "    words = text.split()\n",
    "    embedding = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word in model.wv.key_to_index:\n",
    "            embedding += model.wv[word]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        embedding /= count\n",
    "    return embedding\n",
    "\n",
    "X_train_word2vec = np.array([get_word2vec_embeddings(text, word2vec_model, 100) for text in X_train])\n",
    "X_test_word2vec = np.array([get_word2vec_embeddings(text, word2vec_model, 100) for text in X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f99c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "X_train_bert = np.array([get_bert_embeddings(text, tokenizer, model) for text in X_train])\n",
    "X_test_bert = np.array([get_bert_embeddings(text, tokenizer, model) for text in X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5c57f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9096, 16569) (9096, 16569) (9096, 100) (9096, 768)\n"
     ]
    }
   ],
   "source": [
    "# Ensure you have the correct shapes\n",
    "print(X_train_tfidf.shape, X_train_count.shape, X_train_word2vec.shape, X_train_bert.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653a586",
   "metadata": {},
   "source": [
    "### Addressing Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf33357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_train: [0 1]\n",
      "Updated class weights dict: {0: 0.6164272160477094, 1: 2.647264260768335}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Ensure y_train and y_test are numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Check unique values in y_train\n",
    "unique_classes = np.unique(y_train)\n",
    "print(\"Unique classes in y_train:\", unique_classes)\n",
    "\n",
    "# Compute class weights correctly\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=y_train)\n",
    "class_weights_dict = dict(zip(unique_classes, class_weights))\n",
    "print(\"Updated class weights dict:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79a82e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8601526e",
   "metadata": {},
   "source": [
    "### Feed forward Neural Network(FFNN) Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "295a9e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "    \n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be2740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ffnn(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(input_shape,), activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[F1Score()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fb50a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aravinthkumar\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - f1_score: 0.4436 - loss: 0.6094 - val_f1_score: 0.6243 - val_loss: 0.3980\n",
      "Epoch 2/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - f1_score: 0.8444 - loss: 0.1925 - val_f1_score: 0.6393 - val_loss: 0.3653\n",
      "Epoch 3/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - f1_score: 0.9452 - loss: 0.0666 - val_f1_score: 0.6642 - val_loss: 0.3828\n",
      "Epoch 4/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - f1_score: 0.9853 - loss: 0.0222 - val_f1_score: 0.6474 - val_loss: 0.4326\n",
      "Epoch 5/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - f1_score: 0.9883 - loss: 0.0152 - val_f1_score: 0.6634 - val_loss: 0.4688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1988dada710>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using TF-IDF vectorized data\n",
    "input_shape = X_train_tfidf.shape[1]\n",
    "model_tfidf = create_ffnn(input_shape)\n",
    "\n",
    "# Fit the model with class weights\n",
    "model_tfidf.fit(X_train_tfidf.toarray(), y_train, epochs=10, batch_size=32, validation_data=(X_test_tfidf.toarray(), y_test), class_weight=class_weights_dict, callbacks=[EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa4296f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - f1_score: 0.4741 - loss: 0.5836 - val_f1_score: 0.6177 - val_loss: 0.4347\n",
      "Epoch 2/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - f1_score: 0.8453 - loss: 0.1826 - val_f1_score: 0.6917 - val_loss: 0.3159\n",
      "Epoch 3/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - f1_score: 0.9655 - loss: 0.0460 - val_f1_score: 0.6810 - val_loss: 0.3738\n",
      "Epoch 4/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - f1_score: 0.9850 - loss: 0.0201 - val_f1_score: 0.7051 - val_loss: 0.4249\n",
      "Epoch 5/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - f1_score: 0.9950 - loss: 0.0088 - val_f1_score: 0.7000 - val_loss: 0.4604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x198a7e93370>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using CountVectorizer data\n",
    "input_shape = X_train_count.shape[1]\n",
    "model_count = create_ffnn(input_shape)\n",
    "\n",
    "# Fit the model with class weights\n",
    "model_count.fit(X_train_count.toarray(), y_train, epochs=10, batch_size=32, validation_data=(X_test_count.toarray(), y_test), class_weight=class_weights_dict, callbacks=[EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c864865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - f1_score: 0.4526 - loss: 0.5953 - val_f1_score: 0.5349 - val_loss: 0.4682\n",
      "Epoch 2/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - f1_score: 0.5230 - loss: 0.5516 - val_f1_score: 0.5050 - val_loss: 0.5287\n",
      "Epoch 3/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - f1_score: 0.5325 - loss: 0.5333 - val_f1_score: 0.5355 - val_loss: 0.4395\n",
      "Epoch 4/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - f1_score: 0.5313 - loss: 0.5263 - val_f1_score: 0.5383 - val_loss: 0.4593\n",
      "Epoch 5/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - f1_score: 0.5154 - loss: 0.5453 - val_f1_score: 0.5025 - val_loss: 0.5322\n",
      "Epoch 6/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - f1_score: 0.5321 - loss: 0.5341 - val_f1_score: 0.5204 - val_loss: 0.5122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x198aa5a19f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Word2Vec embeddings\n",
    "input_shape = X_train_word2vec.shape[1]\n",
    "model_word2vec = create_ffnn(input_shape)\n",
    "\n",
    "# Fit the model with class weights\n",
    "model_word2vec.fit(X_train_word2vec, y_train, epochs=10, batch_size=32, validation_data=(X_test_word2vec, y_test), class_weight=class_weights_dict, callbacks=[EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "670faa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - f1_score: 0.5486 - loss: 0.4962 - val_f1_score: 0.6006 - val_loss: 0.4273\n",
      "Epoch 2/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - f1_score: 0.6299 - loss: 0.4027 - val_f1_score: 0.5973 - val_loss: 0.4311\n",
      "Epoch 3/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - f1_score: 0.6498 - loss: 0.3742 - val_f1_score: 0.5810 - val_loss: 0.4909\n",
      "Epoch 4/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - f1_score: 0.6714 - loss: 0.3391 - val_f1_score: 0.6086 - val_loss: 0.4197\n",
      "Epoch 5/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - f1_score: 0.7021 - loss: 0.3057 - val_f1_score: 0.6131 - val_loss: 0.3995\n",
      "Epoch 6/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - f1_score: 0.7520 - loss: 0.2538 - val_f1_score: 0.6206 - val_loss: 0.4015\n",
      "Epoch 7/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - f1_score: 0.7901 - loss: 0.2214 - val_f1_score: 0.6320 - val_loss: 0.3769\n",
      "Epoch 8/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - f1_score: 0.8131 - loss: 0.1844 - val_f1_score: 0.6129 - val_loss: 0.4727\n",
      "Epoch 9/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - f1_score: 0.8350 - loss: 0.1570 - val_f1_score: 0.6288 - val_loss: 0.4365\n",
      "Epoch 10/10\n",
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - f1_score: 0.8861 - loss: 0.1182 - val_f1_score: 0.6434 - val_loss: 0.4538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19892ec75b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using BERT embeddings\n",
    "input_shape = X_train_bert.shape[1]\n",
    "model_bert = create_ffnn(input_shape)\n",
    "\n",
    "# Fit the model with class weights\n",
    "model_bert.fit(X_train_bert, y_train, epochs=10, batch_size=32, validation_data=(X_test_bert, y_test), class_weight=class_weights_dict, callbacks=[EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2c3d9",
   "metadata": {},
   "source": [
    "### Evaluate Models and Compile Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8046d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to calculate evaluation metrics\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred_prob = model.predict(X_test).ravel()\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49388d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store the metrics for each model\n",
    "metrics = {\n",
    "    'Model': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1-Score': []\n",
    "}\n",
    "\n",
    "# Evaluate the TF-IDF model\n",
    "tfidf_metrics = evaluate_model(model_tfidf, X_test_tfidf.toarray(), y_test)\n",
    "metrics['Model'].append('TF-IDF')\n",
    "for metric, value in tfidf_metrics.items():\n",
    "    metrics[metric].append(value)\n",
    "\n",
    "# Evaluate the CountVectorizer model\n",
    "count_metrics = evaluate_model(model_count, X_test_count.toarray(), y_test)\n",
    "metrics['Model'].append('CountVectorizer')\n",
    "for metric, value in count_metrics.items():\n",
    "    metrics[metric].append(value)\n",
    "\n",
    "# Evaluate the Word2Vec model\n",
    "word2vec_metrics = evaluate_model(model_word2vec, X_test_word2vec, y_test)\n",
    "metrics['Model'].append('Word2Vec')\n",
    "for metric, value in word2vec_metrics.items():\n",
    "    metrics[metric].append(value)\n",
    "\n",
    "# Evaluate the BERT model\n",
    "bert_metrics = evaluate_model(model_bert, X_test_bert, y_test)\n",
    "metrics['Model'].append('BERT')\n",
    "for metric, value in bert_metrics.items():\n",
    "    metrics[metric].append(value)\n",
    "\n",
    "# Create a DataFrame from the metrics dictionary\n",
    "metrics_df = pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d087ec3f",
   "metadata": {},
   "source": [
    "## FFNN Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2783757c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.878628</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.686869</td>\n",
       "      <td>0.663415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.905013</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.772208</td>\n",
       "      <td>0.410819</td>\n",
       "      <td>0.709596</td>\n",
       "      <td>0.520370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.856201</td>\n",
       "      <td>0.566219</td>\n",
       "      <td>0.744949</td>\n",
       "      <td>0.643402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Accuracy  Precision    Recall  F1-Score\n",
       "0           TF-IDF  0.878628   0.641509  0.686869  0.663415\n",
       "1  CountVectorizer  0.905013   0.777778  0.636364  0.700000\n",
       "2         Word2Vec  0.772208   0.410819  0.709596  0.520370\n",
       "3             BERT  0.856201   0.566219  0.744949  0.643402"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
